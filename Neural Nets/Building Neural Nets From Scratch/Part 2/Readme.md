## Perceptron optimization, learning, gradient&nbsp;descent&nbsp;

In the last article we understood what perceptrons are and how to model a binary classification task using perceptrons. We understoo the importance of the parameters weights w1, w2 and bias b and solved the problem of classifying outputs of an AND gate.

 ![](https://cdn-images-1.medium.com/max/1200/1*83ZjwYHc4fbgnJgrcqmX8g.png)

We hand crafted the parameters the last time around and we left off with the note that we need an automated approach or an algorithm which when fed the data would find out the values for these parameters. Here is the problem statement in short which we set out to solve in the last blog, given the data set of university acceptance based on test score and grades, we need to find the straight line decision boundary which separates the student who were accepted from those who were rejected.

> Thought worm&nbsp;: What if the relationship between inputs and the output is complex and they cannot be separated by a straight line?


